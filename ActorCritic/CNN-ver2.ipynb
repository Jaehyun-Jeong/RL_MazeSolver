{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52231de0-6a6a-4d14-a8e4-5685bc23b1a5",
   "metadata": {},
   "source": [
    "# Actor-Critic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4ed9c2-2df0-4dcd-8904-a02d499ecbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1c42de-fd2f-4fc2-830f-f50cba650779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5634a10-173a-4b86-8dd1-0f121daea17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.actor_bn1 = nn.BatchNorm2d(16)\n",
    "        self.actor_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn2 = nn.BatchNorm2d(32)\n",
    "        self.actor_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn3 = nn.BatchNorm2d(32)\n",
    "        self.actor_bn4 = nn.BatchNorm1d(4) # 4 actions\n",
    "        self.actor_tanh = nn.Tanh()\n",
    "        \n",
    "        self.critic_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.critic_bn1 = nn.BatchNorm2d(16)\n",
    "        self.critic_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn2 = nn.BatchNorm2d(32)\n",
    "        self.critic_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn3 = nn.BatchNorm2d(32)\n",
    "        self.critic_bn4 = nn.BatchNorm1d(1)\n",
    "        self.critic_tanh = nn.Tanh()\n",
    "        \n",
    "        # torch.log makes nan(not a number) error so we have to add some small number in log function\n",
    "        self.ups=1e-7\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.actor_fc1 = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.critic_fc1 = nn.Linear(linear_input_size, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        state = x.to(device)\n",
    "        \n",
    "        probs = F.relu(self.actor_conv1(state))\n",
    "        probs = F.relu(self.actor_conv2(probs))\n",
    "        probs = F.relu(self.actor_conv3(probs))\n",
    "        probs = torch.flatten(probs, 1)\n",
    "        probs = self.head(self.actor_fc1(probs))\n",
    "        \n",
    "        value = F.relu(self.critic_conv1(state))\n",
    "        value = F.relu(self.critic_conv2(value))\n",
    "        value = F.relu(self.critic_conv3(value))\n",
    "        value = torch.flatten(value, 1)\n",
    "        value = F.relu(self.critic_fc1(value))\n",
    "        \n",
    "        return value, probs\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        action = action.data\n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "\n",
    "    def pi(self, s, a):\n",
    "        s = torch.Tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        _, probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        return probs[a]\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon = 0.1):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        \n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            action = torch.tensor([torch.argmax(probs)])\n",
    "        else:\n",
    "            action = torch.rand(probs.shape).multinomial(num_samples=1)\n",
    "        \n",
    "        action = action.data\n",
    "        action = action[0]\n",
    "        return action\n",
    "    \n",
    "    def value(self, s):\n",
    "        s = torch.tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        value, _ = self.forward(s)\n",
    "        value = torch.squeeze(value, 0)\n",
    "        value = value[0]\n",
    "        \n",
    "        return value\n",
    "    \n",
    "def update_weight(optimizer, states, actions, rewards, last_state, entropy_term=0):\n",
    "    # compute Q values\n",
    "    Qval = actor_critic.value(last_state)\n",
    "    loss = torch.tensor(0, dtype=torch.float32).to(device)\n",
    "    # loss obtained when rewards are obtained\n",
    "    len_loss = len(rewards)\n",
    "    \n",
    "    for s_t, a_t, r_tt in reversed(list(zip(states, actions, rewards))):\n",
    "        log_prob = torch.log(actor_critic.pi(s_t, a_t))\n",
    "        value = actor_critic.value(s_t)\n",
    "        Qval = r_tt + GAMMA * torch.clone(Qval)\n",
    "        \n",
    "        advantage = Qval - value\n",
    "        \n",
    "        actor_loss = (-log_prob * advantage)\n",
    "        critic_loss = 0.5 * advantage.pow(2)\n",
    "        \n",
    "        loss += actor_loss + critic_loss\n",
    "        \n",
    "    loss = loss/len_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd3f1-4262-4944-bc96-ce35e6aa3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 return -536.0000000000069\n",
      "Episode 20 return -543.0000000000064\n",
      "Episode 30 return -592.0000000000024\n",
      "Episode 40 return -551.0000000000061\n",
      "Episode 50 return -547.500000000007\n",
      "Episode 60 return -510.5000000000088\n",
      "Episode 70 return -563.0000000000043\n",
      "Episode 80 return -537.0000000000063\n",
      "Episode 90 return -560.5000000000048\n",
      "Episode 100 return -552.5000000000052\n",
      "Episode 110 return -546.0000000000059\n",
      "Episode 120 return -554.5000000000052\n",
      "Episode 130 return -524.5000000000076\n",
      "Episode 140 return -526.5000000000075\n",
      "Episode 150 return -505.00000000000875\n",
      "Episode 160 return -639.0\n",
      "Episode 170 return -721.4999999999958\n",
      "Episode 180 return -789.999999999993\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "ALPHA = 3e-4 # learning rate\n",
    "GAMMA = 0.99 # step-size\n",
    "\n",
    "env = MazeSolverEnv()\n",
    "\n",
    "def train_ActorCritic(env):\n",
    "\n",
    "    num_actions = env.num_action\n",
    "    num_states = env.num_obs\n",
    "\n",
    "    actor_critic = ActorCritic(num_states[0], num_states[1], num_actions).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(actor_critic.parameters(), lr=ALPHA)\n",
    "\n",
    "    try:\n",
    "        returns = []\n",
    "\n",
    "        for i_episode in range(MAX_EPISODES):\n",
    "\n",
    "            state = env.init_obs\n",
    "            init_state = state\n",
    "\n",
    "            done = False\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []   # no reward at t = 0\n",
    "\n",
    "            #while not done:\n",
    "            for timesteps in range(MAX_TIMESTEPS):\n",
    "\n",
    "                states.append(state)\n",
    "\n",
    "                action = actor_critic.get_action(state)\n",
    "                actions.append(action)\n",
    "\n",
    "                state, reward, done, _ = env.step(action.tolist())\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if done or timesteps == MAX_TIMESTEPS-1:\n",
    "                    last_state = state\n",
    "                    break\n",
    "\n",
    "            update_weight(optimizer, states, actions, rewards, last_state)\n",
    "\n",
    "            '''\n",
    "            #====================================================================================================\n",
    "            #to see the change of the weights====================================================================\n",
    "            #====================================================================================================\n",
    "            print(\"=========================================================================================\")\n",
    "            print(\"actor_critic.actor_fc1.weight : {}\".format(actor_critic.actor_conv1.weight))\n",
    "            print(\"actor_critic.critic_fc2.weight : {}\".format(actor_critic.critic_conv1.weight))\n",
    "            #====================================================================================================\n",
    "            '''\n",
    "\n",
    "            if (i_episode + 1) % 500 == 0:\n",
    "                print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "                returns.append(sum(rewards))\n",
    "                torch.save(actor_critic, './saved_models/actor_critic' + str(i_episode + 1) + '.pt')\n",
    "            elif (i_episode + 1) % 10 == 0:\n",
    "                print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "                returns.append(sum(rewards))\n",
    "\n",
    "            # every 10th episodes, turn off the exploring starts\n",
    "            if (i_episode + 2) % 10 == 0:\n",
    "                env.reset_player(exploring_starts = False)\n",
    "            else:\n",
    "                env.reset_player(exploring_starts = True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "    finally:\n",
    "        plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
