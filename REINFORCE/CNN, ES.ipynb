{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c747e488-77a1-470e-8e24-29dde01c68d1",
   "metadata": {},
   "source": [
    "# REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e9aecd-fc49-4fbd-8c79-54e510f0592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Monte-Carlo Policy Gradient \"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f222884f-e29d-4136-bfd6-e6f726df3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ae973b-d0d4-49ef-959f-898038076405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforce(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(reinforce, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.bn4 = nn.BatchNorm1d(4) # 4 actions\n",
    "        \n",
    "        # torch.log makes nan(not a number) error so we have to add some small number in log function\n",
    "        self.ups=1e-7\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Softmax(dim=1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        \n",
    "        return self.head(x)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = Variable(torch.tensor(state))\n",
    "        #state = torch.unsqueeze(state, 0)\n",
    "        probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=4)\n",
    "        action = action.data\n",
    "        \n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "\n",
    "    def pi(self, s, a):\n",
    "        s = Variable(torch.Tensor(s))\n",
    "        probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        return probs[a]\n",
    "\n",
    "    def update_weight(self, states, actions, rewards, optimizer):\n",
    "        G = Variable(torch.Tensor([0])).to(device)\n",
    "        # for each step of the episode t = T - 1, ..., 0\n",
    "        # r_tt represents r_{t+1}\n",
    "        \n",
    "        for s_t, a_t, r_tt in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
    "            G = Variable(torch.Tensor([r_tt])).to(device) + GAMMA * G\n",
    "            loss = (-1.0) * G * torch.log(self.pi(s_t, a_t) + self.ups).to(device)\n",
    "            # update policy parameter \\theta\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe24d27-aede-4102-8ca2-c7619e7188ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "0\n",
      "2\n",
      "=========================\n",
      "[1 2]\n",
      "1\n",
      "2\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 1, 3]])\n",
    "\n",
    "for i in np.argwhere(a == 3):\n",
    "    print(i)\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    \n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c55ac5-e353-4275-843e-ef3cad8a0284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 timesteps 2\n",
      "Update 100 finished\n",
      "Episode 200 timesteps 1\n",
      "Update 200 finished\n",
      "Episode 300 timesteps 5\n",
      "Update 300 finished\n",
      "Episode 400 timesteps 4\n",
      "Update 400 finished\n",
      "Episode 500 timesteps 1\n",
      "Update 500 finished\n",
      "Episode 600 timesteps 2\n",
      "Update 600 finished\n",
      "Episode 700 timesteps 1\n",
      "Update 700 finished\n",
      "Episode 800 timesteps 3\n",
      "Update 800 finished\n",
      "Episode 900 timesteps 1\n",
      "Update 900 finished\n",
      "Episode 1000 timesteps 2\n",
      "Update 1000 finished\n",
      "Episode 1100 timesteps 5\n",
      "Update 1100 finished\n",
      "Episode 1200 timesteps 2\n",
      "Update 1200 finished\n",
      "Episode 1300 timesteps 2\n",
      "Update 1300 finished\n",
      "Episode 1400 timesteps 1\n",
      "Update 1400 finished\n",
      "Episode 1500 timesteps 1\n",
      "Update 1500 finished\n",
      "Episode 1600 timesteps 3\n",
      "Update 1600 finished\n",
      "Episode 1700 timesteps 1\n",
      "Update 1700 finished\n",
      "Episode 1800 timesteps 5\n",
      "Update 1800 finished\n",
      "Episode 1900 timesteps 2\n",
      "Update 1900 finished\n",
      "Episode 2000 timesteps 1\n",
      "Update 2000 finished\n",
      "Episode 2100 timesteps 2\n",
      "Update 2100 finished\n",
      "Episode 2200 timesteps 3\n",
      "Update 2200 finished\n",
      "Episode 2300 timesteps 1\n",
      "Update 2300 finished\n",
      "Episode 2400 timesteps 2\n",
      "Update 2400 finished\n",
      "Episode 2500 timesteps 3\n",
      "Update 2500 finished\n",
      "Episode 2600 timesteps 1\n",
      "Update 2600 finished\n",
      "Episode 2700 timesteps 1\n",
      "Update 2700 finished\n",
      "Episode 2800 timesteps 2\n",
      "Update 2800 finished\n",
      "Episode 2900 timesteps 1\n",
      "Update 2900 finished\n",
      "Episode 3000 timesteps 2\n",
      "Update 3000 finished\n",
      "Episode 3100 timesteps 2\n",
      "Update 3100 finished\n",
      "Episode 3200 timesteps 4\n",
      "Update 3200 finished\n",
      "Episode 3300 timesteps 1\n",
      "Update 3300 finished\n",
      "Episode 3400 timesteps 2\n",
      "Update 3400 finished\n",
      "Episode 3500 timesteps 2\n",
      "Update 3500 finished\n",
      "Episode 3600 timesteps 4\n",
      "Update 3600 finished\n",
      "Episode 3700 timesteps 3\n",
      "Update 3700 finished\n",
      "Episode 3800 timesteps 2\n",
      "Update 3800 finished\n",
      "Episode 3900 timesteps 1\n",
      "Update 3900 finished\n",
      "Episode 4000 timesteps 1\n",
      "Update 4000 finished\n",
      "Episode 4100 timesteps 2\n",
      "Update 4100 finished\n",
      "Episode 4200 timesteps 3\n",
      "Update 4200 finished\n",
      "Episode 4300 timesteps 2\n",
      "Update 4300 finished\n",
      "Episode 4400 timesteps 2\n",
      "Update 4400 finished\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 100000\n",
    "MAX_TIMESTEPS = 100000\n",
    "\n",
    "ALPHA = 3e-5\n",
    "GAMMA = 0.99\n",
    "\n",
    "env = MazeSolverEnv()\n",
    "\n",
    "# for exploring starts and randomized goal\n",
    "env.reset(exploring_starts = True, random_goal = True)\n",
    "\n",
    "num_actions = env.num_action\n",
    "num_obs = env.num_obs\n",
    "\n",
    "agent = reinforce(num_obs[0], num_obs[1], num_actions).to(device)\n",
    "agent.eval()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=ALPHA)\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    state = env.init_obs\n",
    "    done = False\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = [0]   # no reward at t = 0\n",
    "    \n",
    "    #while not done:\n",
    "    for timesteps in range(MAX_TIMESTEPS):\n",
    "        action = agent.get_action(state)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        state, reward, done, _ = env.step(action.tolist())\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            if (i_episode+1) % 100 == 0:\n",
    "                print(\"Episode {} timesteps {}\".format(i_episode + 1, timesteps + 1))\n",
    "            #print(\"Episode {} finished after {} timesteps\".format(i_episode, timesteps+1))\n",
    "            break\n",
    "                \n",
    "    agent.update_weight(states, actions, rewards, optimizer)\n",
    "    \n",
    "    if (i_episode+1) % 100 == 0:\n",
    "        print(\"Update {} finished\".format(i_episode + 1))\n",
    "        \n",
    "    \n",
    "    env.reset(exploring_starts = True, random_goal = True)\n",
    "\n",
    "env.close()\n",
    "torch.save(agent, './saved_models/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
