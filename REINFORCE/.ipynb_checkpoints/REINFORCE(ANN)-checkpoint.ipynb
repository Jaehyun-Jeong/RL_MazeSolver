{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c747e488-77a1-470e-8e24-29dde01c68d1",
   "metadata": {},
   "source": [
    "# REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e9aecd-fc49-4fbd-8c79-54e510f0592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Monte-Carlo Policy Gradient \"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f222884f-e29d-4136-bfd6-e6f726df3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ae973b-d0d4-49ef-959f-898038076405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforce(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(reinforce, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "        self.head = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.ups=1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.head(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        #state = torch.unsqueeze(state, 0)\n",
    "        probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        action = action.data\n",
    "        \n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon = 0.1):\n",
    "        state = torch.tensor(state)\n",
    "        #state = torch.unsqueeze(state, 0)\n",
    "        probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            action = torch.tensor([torch.argmax(probs)])\n",
    "        else:\n",
    "            action = torch.rand(probs.shape).multinomial(num_samples=1)\\\n",
    "        \n",
    "        action = action.data\n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "        \n",
    "    def pi(self, s, a):\n",
    "        s = torch.Tensor(s)\n",
    "        probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        return probs[a]\n",
    "\n",
    "    def update_weight(self, states, actions, rewards, optimizer):\n",
    "        G = torch.Tensor([0])\n",
    "        # for each step of the episode t = T - 1, ..., 0\n",
    "        # r_tt represents r_{t+1}\n",
    "        \n",
    "        for s_t, a_t, r_tt in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
    "            G = torch.Tensor([r_tt]) + GAMMA * G\n",
    "            loss = (-1.0) * G * torch.log(self.pi(s_t, a_t) + self.ups)\n",
    "            # update policy parameter \\theta\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c55ac5-e353-4275-843e-ef3cad8a0284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 return: 9.0\n",
      "Episode 200 return: 9.0\n",
      "Episode 300 return: 10.0\n",
      "Episode 400 return: 13.0\n",
      "Episode 500 return: 10.0\n",
      "Episode 600 return: 11.0\n",
      "Episode 700 return: 9.0\n",
      "Episode 800 return: 10.0\n",
      "Episode 900 return: 9.0\n",
      "Episode 1000 return: 10.0\n",
      "Episode 1100 return: 9.0\n",
      "Episode 1200 return: 9.0\n",
      "Episode 1300 return: 10.0\n",
      "Episode 1400 return: 11.0\n",
      "Episode 1500 return: 10.0\n",
      "Episode 1600 return: 10.0\n",
      "Episode 1700 return: 10.0\n",
      "Episode 1800 return: 10.0\n",
      "Episode 1900 return: 11.0\n",
      "Episode 2000 return: 10.0\n",
      "Episode 2100 return: 9.0\n",
      "Episode 2200 return: 10.0\n",
      "Episode 2300 return: 8.0\n",
      "Episode 2400 return: 8.0\n",
      "Episode 2500 return: 10.0\n",
      "Episode 2600 return: 10.0\n",
      "Episode 2700 return: 10.0\n",
      "Episode 2800 return: 10.0\n",
      "Episode 2900 return: 14.0\n",
      "Episode 3000 return: 10.0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 3000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "ALPHA = 3e-5\n",
    "GAMMA = 0.99\n",
    "\n",
    "#env = MazeSolverEnv()\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#num_actions = env.num_action\n",
    "num_actions =  env.action_space.n\n",
    "num_states = env.observation_space.shape[0]\n",
    "\n",
    "agent = reinforce(num_states, num_actions).to(device)\n",
    "agent.eval()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=ALPHA)\n",
    "\n",
    "returns = []\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    #state = env.init_obs\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = [0]   # no reward at t = 0\n",
    "    \n",
    "    #while not done:\n",
    "    for timestep in range(MAX_TIMESTEPS):\n",
    "        action = agent.epsilon_greedy_action(state)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action.tolist())\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done or (timestep+1) == MAX_TIMESTEPS:\n",
    "            #print(\"Episode {} finished after {} timesteps\".format(i_episode, timesteps+1))\n",
    "            break\n",
    "            \n",
    "    returns.append(rewards)\n",
    "    \n",
    "    if (i_episode+1) % 100 == 0:\n",
    "        print(\"Episode {} return: {}\".format( i_episode + 1, sum(rewards)))\n",
    "    \n",
    "    agent.update_weight(states, actions, rewards, optimizer)\n",
    "        \n",
    "    if (i_episode + 1) % 500 == 0:\n",
    "        torch.save(agent, './saved_models/model' + str(i_episode + 1) + '.pt')\n",
    "    \n",
    "    #env.reset_player(exploring_starts = False)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
