{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52231de0-6a6a-4d14-a8e4-5685bc23b1a5",
   "metadata": {},
   "source": [
    "# Actor-Critic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4ed9c2-2df0-4dcd-8904-a02d499ecbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1c42de-fd2f-4fc2-830f-f50cba650779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5634a10-173a-4b86-8dd1-0f121daea17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.actor_bn1 = nn.BatchNorm2d(16)\n",
    "        self.actor_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn2 = nn.BatchNorm2d(32)\n",
    "        self.actor_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn3 = nn.BatchNorm2d(32)\n",
    "        self.actor_bn4 = nn.BatchNorm1d(4) # 4 actions\n",
    "        self.actor_tanh = nn.Tanh()\n",
    "        \n",
    "        self.critic_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.critic_bn1 = nn.BatchNorm2d(16)\n",
    "        self.critic_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn2 = nn.BatchNorm2d(32)\n",
    "        self.critic_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn3 = nn.BatchNorm2d(32)\n",
    "        self.critic_bn4 = nn.BatchNorm1d(1)\n",
    "        self.critic_tanh = nn.Tanh()\n",
    "        \n",
    "        # torch.log makes nan(not a number) error so we have to add some small number in log function\n",
    "        self.ups=1e-7\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.actor_fc1 = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.critic_fc1 = nn.Linear(linear_input_size, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        state = x.to(device)\n",
    "        \n",
    "        probs = F.relu(self.actor_conv1(state))\n",
    "        probs = F.relu(self.actor_conv2(probs))\n",
    "        probs = F.relu(self.actor_conv3(probs))\n",
    "        probs = torch.flatten(probs, 1)\n",
    "        probs = self.head(self.actor_fc1(probs))\n",
    "        \n",
    "        value = F.relu(self.critic_conv1(state))\n",
    "        value = F.relu(self.critic_conv2(value))\n",
    "        value = F.relu(self.critic_conv3(value))\n",
    "        value = torch.flatten(value, 1)\n",
    "        value = F.relu(self.critic_fc1(value))\n",
    "        \n",
    "        return value, probs\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        action = action.data\n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "\n",
    "    def pi(self, s, a):\n",
    "        s = torch.Tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        _, probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        return probs[a]\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon = 0.1):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        \n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            action = torch.tensor([torch.argmax(probs)])\n",
    "        else:\n",
    "            action = torch.rand(probs.shape).multinomial(num_samples=1)\n",
    "        \n",
    "        action = action.data\n",
    "        action = action[0]\n",
    "        return action\n",
    "    \n",
    "    def value(self, s):\n",
    "        s = torch.tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        value, _ = self.forward(s)\n",
    "        value = torch.squeeze(value, 0)\n",
    "        value = value[0]\n",
    "        \n",
    "        return value\n",
    "    \n",
    "def update_weight(optimizer, values, log_probs, rewards, last_value, entropy_term = 0):\n",
    "    # compute Q values\n",
    "    Qval = actor_critic.value(last_state)\n",
    "    loss = 0\n",
    "    \n",
    "    for s_t, a_t, r_tt in reversed(list(zip(states, actions, rewards))):\n",
    "        log_prob = torch.log(actor_critic.pi(s_t, a_t))\n",
    "        value = actor_critic.value(s_t)\n",
    "        Qval = r_tt + GAMMA * torch.clone(Qval)\n",
    "        \n",
    "        advantage = Qval - value\n",
    "\n",
    "        actor_loss = (-log_prob * advantage)\n",
    "        critic_loss = 0.5 * advantage.pow(2)\n",
    "        loss += actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd3f1-4262-4944-bc96-ce35e6aa3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 return -534.5000000000073\n",
      "Episode 20 return -544.0000000000065\n",
      "Episode 30 return -526.0000000000077\n",
      "Episode 40 return -534.0000000000072\n",
      "Episode 50 return -536.5000000000067\n",
      "Episode 60 return -295.41000000000037\n",
      "Episode 70 return -533.5000000000074\n",
      "Episode 80 return -545.500000000006\n",
      "Episode 90 return -553.5000000000051\n",
      "Episode 100 return -535.0000000000074\n",
      "Episode 110 return -549.0000000000059\n",
      "Episode 120 return -524.5000000000076\n",
      "Episode 130 return -532.1700000000073\n",
      "Episode 140 return -539.000000000006\n",
      "Episode 150 return -594.5000000000028\n",
      "Episode 160 return -641.0000000000001\n",
      "Episode 170 return -676.9999999999983\n",
      "Episode 180 return -526.0000000000074\n",
      "Episode 190 return -490.50000000000716\n",
      "Episode 200 return -530.5000000000065\n",
      "Episode 210 return -544.5000000000067\n",
      "Episode 220 return -565.0000000000045\n",
      "Episode 230 return -563.5000000000042\n",
      "Episode 240 return -526.000000000008\n",
      "Episode 250 return -540.500000000006\n",
      "Episode 260 return -592.0000000000036\n",
      "Episode 270 return -584.500000000003\n",
      "Episode 280 return -571.5000000000045\n",
      "Episode 290 return -516.0000000000084\n",
      "Episode 300 return -377.50000000000387\n",
      "Episode 310 return -539.5000000000068\n",
      "Episode 320 return -194.57999999999961\n",
      "Episode 330 return -533.0000000000067\n",
      "Episode 340 return -245.04999999999887\n",
      "Episode 350 return -538.5000000000066\n",
      "Episode 360 return -537.5000000000065\n",
      "Episode 370 return -401.8400000000046\n",
      "Episode 380 return -272.86999999999955\n",
      "Episode 390 return -458.9600000000073\n",
      "Episode 400 return -525.0000000000078\n",
      "Episode 410 return -540.0000000000059\n",
      "Episode 420 return -552.5000000000052\n",
      "Episode 430 return -537.5000000000069\n",
      "Episode 440 return -374.34000000000367\n",
      "Episode 450 return -522.0000000000077\n",
      "Episode 460 return -448.89000000000664\n",
      "Episode 470 return -531.5000000000074\n",
      "Episode 480 return -534.5000000000068\n",
      "Episode 490 return -124.45000000000067\n",
      "Episode 500 return -428.92000000000564\n",
      "Episode 510 return -506.2000000000091\n",
      "Episode 520 return -566.0000000000039\n",
      "Episode 530 return -390.4100000000042\n",
      "Episode 540 return -511.000000000009\n",
      "Episode 550 return -535.5000000000067\n",
      "Episode 560 return -532.0000000000073\n",
      "Episode 570 return -532.500000000007\n",
      "Episode 580 return -562.500000000005\n",
      "Episode 590 return -515.5000000000092\n",
      "Episode 600 return -529.5000000000075\n",
      "Episode 610 return -558.0000000000053\n",
      "Episode 620 return -534.5000000000072\n",
      "Episode 630 return -541.0000000000061\n",
      "Episode 640 return -551.500000000006\n",
      "Episode 650 return -530.5000000000075\n",
      "Episode 660 return -530.5000000000073\n",
      "Episode 670 return -548.5000000000063\n",
      "Episode 680 return -539.0000000000061\n",
      "Episode 690 return -546.5000000000063\n",
      "Episode 700 return -539.5000000000068\n",
      "Episode 710 return -550.0000000000058\n",
      "Episode 720 return -520.0000000000081\n",
      "Episode 730 return -558.5000000000056\n",
      "Episode 740 return -547.5000000000058\n",
      "Episode 750 return -523.500000000008\n",
      "Episode 760 return -501.00000000000904\n",
      "Episode 770 return -513.000000000009\n",
      "Episode 780 return -541.0000000000064\n",
      "Episode 790 return -523.000000000008\n",
      "Episode 800 return -525.0000000000083\n",
      "Episode 810 return -294.4000000000004\n",
      "Episode 820 return -544.5000000000061\n",
      "Episode 830 return -549.0000000000056\n",
      "Episode 840 return -560.0000000000049\n",
      "Episode 850 return -531.0000000000073\n",
      "Episode 860 return -514.5000000000086\n",
      "Episode 870 return -524.0000000000081\n",
      "Episode 880 return -532.5000000000072\n",
      "Episode 890 return -532.5000000000073\n",
      "Episode 900 return -545.5000000000066\n",
      "Episode 910 return -510.50000000000915\n",
      "Episode 920 return -548.0000000000061\n",
      "Episode 930 return -528.5000000000075\n",
      "Episode 940 return -510.500000000009\n",
      "Episode 950 return -527.0000000000076\n",
      "Episode 960 return -543.500000000006\n",
      "Episode 970 return -553.5000000000049\n",
      "Episode 980 return -539.0000000000069\n",
      "Episode 990 return -533.0000000000068\n",
      "Episode 1000 return -537.5000000000068\n",
      "Episode 1010 return -545.5000000000063\n",
      "Episode 1020 return -544.0000000000063\n",
      "Episode 1030 return -546.5000000000059\n",
      "Episode 1040 return -538.5000000000068\n",
      "Episode 1050 return -552.0000000000056\n",
      "Episode 1060 return -525.5000000000074\n",
      "Episode 1070 return -509.00000000000887\n",
      "Episode 1080 return -463.47000000000645\n",
      "Episode 1090 return -546.500000000006\n",
      "Episode 1100 return -521.000000000008\n",
      "Episode 1110 return -460.1100000000068\n",
      "Episode 1120 return -519.5000000000083\n",
      "Episode 1130 return -462.6500000000071\n",
      "Episode 1140 return -532.0000000000067\n",
      "Episode 1150 return -511.00000000000847\n",
      "Episode 1160 return -520.0000000000084\n",
      "Episode 1170 return -536.0000000000066\n",
      "Episode 1180 return -509.00000000000887\n",
      "Episode 1190 return -547.0000000000057\n",
      "Episode 1200 return -552.0000000000056\n",
      "Episode 1210 return -541.5000000000067\n",
      "Episode 1220 return -479.030000000008\n",
      "Episode 1230 return -370.75000000000324\n",
      "Episode 1240 return -498.0000000000088\n",
      "Episode 1250 return -510.50000000000904\n",
      "Episode 1260 return -541.0000000000067\n",
      "Episode 1270 return -531.5000000000075\n",
      "Episode 1280 return -325.81000000000137\n",
      "Episode 1290 return -175.56000000000003\n",
      "Episode 1300 return -505.50000000000904\n",
      "Episode 1310 return -638.0000000000006\n",
      "Episode 1320 return -507.5000000000088\n",
      "Episode 1330 return -401.25000000000466\n",
      "Episode 1340 return -515.5000000000086\n",
      "Episode 1350 return -538.0000000000053\n",
      "Episode 1360 return -585.5000000000031\n",
      "Episode 1370 return -454.0000000000078\n",
      "Episode 1380 return -550.0000000000056\n",
      "Episode 1390 return -670.9999999999977\n",
      "Episode 1400 return -759.999999999994\n",
      "Episode 1410 return -787.9999999999931\n",
      "Episode 1420 return -788.9999999999931\n",
      "Episode 1430 return -787.9999999999931\n",
      "Episode 1440 return -788.9999999999931\n",
      "Episode 1450 return -788.9999999999931\n",
      "Episode 1460 return -788.9999999999931\n",
      "Episode 1470 return -788.9999999999931\n",
      "Episode 1480 return -788.9999999999931\n",
      "Episode 1490 return -788.9999999999931\n",
      "Episode 1500 return -788.9999999999931\n",
      "Episode 1510 return -788.9999999999931\n",
      "Episode 1520 return -788.9999999999931\n",
      "Episode 1530 return -788.9999999999931\n",
      "Episode 1540 return -788.9999999999931\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "ALPHA = 3e-4 # learning rate\n",
    "GAMMA = 0.99 # step-size\n",
    "\n",
    "env = MazeSolverEnv()\n",
    "\n",
    "num_actions = env.num_action\n",
    "num_states = env.num_obs\n",
    "\n",
    "actor_critic = ActorCritic(num_states[0], num_states[1], num_actions).to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=ALPHA)\n",
    "\n",
    "try:\n",
    "    returns = []\n",
    "    \n",
    "    for i_episode in range(MAX_EPISODES):\n",
    "\n",
    "        state = env.init_obs\n",
    "        init_state = state\n",
    "        \n",
    "        done = False\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []   # no reward at t = 0\n",
    "\n",
    "        #while not done:\n",
    "        for timesteps in range(MAX_TIMESTEPS):\n",
    "            \n",
    "            states.append(state)\n",
    "            \n",
    "            action = actor_critic.get_action(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            state, reward, done, _ = env.step(action.tolist())\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done or timesteps == MAX_TIMESTEPS-1:\n",
    "                last_state = state\n",
    "                break\n",
    "        \n",
    "        update_weight(optimizer, states, actions, rewards, last_state)\n",
    "        \n",
    "        '''\n",
    "        #====================================================================================================\n",
    "        #to see the change of the weights====================================================================\n",
    "        #====================================================================================================\n",
    "        print(\"=========================================================================================\")\n",
    "        print(\"actor_critic.actor_fc1.weight : {}\".format(actor_critic.actor_conv1.weight))\n",
    "        print(\"actor_critic.critic_fc2.weight : {}\".format(actor_critic.critic_conv1.weight))\n",
    "        #====================================================================================================\n",
    "        '''\n",
    "\n",
    "        if (i_episode + 1) % 500 == 0:\n",
    "            print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "            returns.append(sum(rewards))\n",
    "            torch.save(actor_critic, './saved_models/actor_critic' + str(i_episode + 1) + '.pt')\n",
    "        elif (i_episode + 1) % 10 == 0:\n",
    "            print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "            returns.append(sum(rewards))\n",
    "        \n",
    "        # every 10th episodes, turn off the exploring starts\n",
    "        if (i_episode + 2) % 10 == 0:\n",
    "            env.reset_player(exploring_starts = False)\n",
    "        else:\n",
    "            env.reset_player(exploring_starts = True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "finally:\n",
    "    plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
