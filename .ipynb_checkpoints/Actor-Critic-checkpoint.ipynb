{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52231de0-6a6a-4d14-a8e4-5685bc23b1a5",
   "metadata": {},
   "source": [
    "# Actor-Critic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4ed9c2-2df0-4dcd-8904-a02d499ecbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1c42de-fd2f-4fc2-830f-f50cba650779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefc17a-e34f-44e0-8c23-f76a93358dbc",
   "metadata": {},
   "source": [
    "![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/sutton_barto_reinforcement_learning%2Fchapter13%2F01.PNG?alt=media&token=7dbd1717-7691-44fc-9375-c86fe3d04e7a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5634a10-173a-4b86-8dd1-0f121daea17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # for Actor\n",
    "        self.fc1 = nn.Linear(inputs, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, inputs)\n",
    "        self.fc4 = nn.Linear(inputs, outputs)\n",
    "        self.bn1 = nn.BatchNorm1d(256) # 4 actions\n",
    "        self.bn2 = nn.BatchNorm1d(inputs)\n",
    "        self.bn3 = nn.BatchNorm1d(outputs)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.head = nn.Softmax(dim=0)\n",
    "        \n",
    "        # torch.log makes nan(not a number) error so we have to add some small number in log function\n",
    "        self.ups=1e-7\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        state = x.to(device)\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        v = self.fc1(state)\n",
    "        v = self.fc2(v)\n",
    "        v = self.fc3(v)\n",
    "        v = self.fc4(v)\n",
    "        \n",
    "        return v, self.head(x)\n",
    "    \n",
    "    def pi(self, s, a):\n",
    "        s = Variable(torch.Tensor(s))\n",
    "        #s = torch.unsqueeze(s, 0)\n",
    "        _, probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        return probs[a]\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = Variable(torch.tensor(state))\n",
    "        #state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        action = action.data\n",
    "        \n",
    "        action = action[0]\n",
    "        return action\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon = 0.1):\n",
    "        state = Variable(torch.tensor(state))\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        \n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            action = torch.tensor([torch.argmax(probs)])\n",
    "        else:\n",
    "            action = torch.rand(probs.shape).multinomial(num_samples=1)\n",
    "        \n",
    "        action = action.data\n",
    "        action = action[0]\n",
    "        return action\n",
    "    \n",
    "    def value(self, s):\n",
    "        s = Variable(torch.tensor(s))\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        value, _ = self.forward(s)\n",
    "        value = torch.squeeze(value, 0)\n",
    "        value = value[0]\n",
    "        \n",
    "        return value\n",
    "'''            \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, inputs)\n",
    "        self.fc4 = nn.Linear(inputs, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128) # 4 actions\n",
    "        self.bn2 = nn.BatchNorm1d(inputs)\n",
    "        self.bn3 = nn.BatchNorm1d(1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "                \n",
    "        return x\n",
    "''' \n",
    "def update_weight(optimizer, values, log_probs, rewards, last_Qval):\n",
    "    \n",
    "    # compute Q values\n",
    "    Qval = last_Qval\n",
    "    Qvals = torch.zeros(len(rewards))\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval\n",
    "        Qvals[t] = Qval\n",
    "    Qvals = Variable(Qvals, requires_grad=True).to(device)\n",
    "    \n",
    "    values = torch.tensor(values, dtype=torch.float32).to(device)\n",
    "    log_probs = torch.tensor(log_probs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss = (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    loss = actor_loss + critic_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd3f1-4262-4944-bc96-ce35e6aa3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500 return 13.0\n",
      "Episode 1000 return 9.0\n",
      "Episode 1500 return 40.0\n",
      "Episode 2000 return 14.0\n",
      "Episode 2500 return 15.0\n",
      "Episode 3000 return 11.0\n",
      "Episode 3500 return 12.0\n",
      "Episode 4000 return 14.0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "ALPHA = 3e-5 # learning rate\n",
    "GAMMA = 0.99 # step-size for actor\n",
    "BETA = 0.9 # step-size for critic\n",
    "\n",
    "#env = MazeSolverEnv()\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "num_actions = 2 #env.num_action\n",
    "num_states = 4 #365\n",
    "\n",
    "actor_critic = ActorCritic(num_states, num_actions).to(device)\n",
    "\n",
    "actor_critic.eval()\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=ALPHA)\n",
    "\n",
    "try:\n",
    "    returns = []\n",
    "    \n",
    "    for i_episode in range(MAX_EPISODES):\n",
    "\n",
    "        state = env.reset()\n",
    "        #state = env.init_obs\n",
    "        done = False\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []   # no reward at t = 0\n",
    "\n",
    "        #while not done:\n",
    "        for timesteps in range(MAX_TIMESTEPS):\n",
    "            value = float(actor_critic.value(state).cpu().detach().numpy())\n",
    "            values.append(value)\n",
    "            action = actor_critic.get_action(state)\n",
    "            log_prob = float(torch.log(actor_critic.pi(state, action)).cpu().detach().numpy())\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, done, _ = env.step(action.tolist())\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done or timesteps == MAX_TIMESTEPS:\n",
    "                last_Qval = float(actor_critic.value(state).cpu().detach().numpy())\n",
    "                #print(\"Episode {} finished after {} timesteps\".format(i_episode, timesteps+1))\n",
    "                break\n",
    "\n",
    "        #print(\"Episode {} return: {}\".format( i_episode + 1, sum(rewards)))\n",
    "\n",
    "        update_weight(optimizer, values, log_probs, rewards, last_Qval)\n",
    "        \n",
    "        #print(\"Update {} finished\".format(i_episode + 1))\n",
    "\n",
    "        if (i_episode + 1) % 500 == 0:\n",
    "            print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "            torch.save(actor_critic, './saved_models/actor_critic' + str(i_episode + 1) + '.pt')\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "        returns.append(sum(rewards))\n",
    "        #env.reset_player(exploring_starts = False)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    plt.plot(range(len(returns)), returns)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
