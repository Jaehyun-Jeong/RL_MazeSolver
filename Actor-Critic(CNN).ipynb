{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52231de0-6a6a-4d14-a8e4-5685bc23b1a5",
   "metadata": {},
   "source": [
    "# Actor-Critic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4ed9c2-2df0-4dcd-8904-a02d499ecbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pygame\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1c42de-fd2f-4fc2-830f-f50cba650779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5634a10-173a-4b86-8dd1-0f121daea17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.actor_bn1 = nn.BatchNorm2d(16)\n",
    "        self.actor_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn2 = nn.BatchNorm2d(32)\n",
    "        self.actor_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.actor_bn3 = nn.BatchNorm2d(32)\n",
    "        self.actor_bn4 = nn.BatchNorm1d(4) # 4 actions\n",
    "        self.actor_tanh = nn.Tanh()\n",
    "        \n",
    "        self.critic_conv1 = nn.Conv2d(5, 16, kernel_size=3, stride=1)\n",
    "        self.critic_bn1 = nn.BatchNorm2d(16)\n",
    "        self.critic_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn2 = nn.BatchNorm2d(32)\n",
    "        self.critic_conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.critic_bn3 = nn.BatchNorm2d(32)\n",
    "        self.critic_bn4 = nn.BatchNorm1d(1)\n",
    "        self.critic_tanh = nn.Tanh()\n",
    "        \n",
    "        # torch.log makes nan(not a number) error so we have to add some small number in log function\n",
    "        self.ups=1e-7\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.actor_fc1 = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.critic_fc1 = nn.Linear(linear_input_size, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        state = x.to(device)\n",
    "        \n",
    "        probs = F.relu(self.actor_conv1(state))\n",
    "        probs = F.relu(self.actor_conv2(probs))\n",
    "        probs = F.relu(self.actor_conv3(probs))\n",
    "        probs = torch.flatten(probs, 1)\n",
    "        probs = self.head(self.actor_fc1(probs))\n",
    "        \n",
    "        value = F.relu(self.critic_conv1(state))\n",
    "        value = F.relu(self.critic_conv2(value))\n",
    "        value = F.relu(self.critic_conv3(value))\n",
    "        value = torch.flatten(value, 1)\n",
    "        value = F.relu(self.critic_fc1(value))\n",
    "        \n",
    "        return value, probs\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        action = action.data\n",
    "        action = action[0].to(device)\n",
    "        return action\n",
    "\n",
    "    def pi(self, s, a):\n",
    "        s = torch.Tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        _, probs = self.forward(s)\n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        return probs[a]\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon = 0.1):\n",
    "        state = torch.tensor(state)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        _, probs = self.forward(state)\n",
    "        \n",
    "        probs = torch.squeeze(probs, 0)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            action = torch.tensor([torch.argmax(probs)])\n",
    "        else:\n",
    "            action = torch.rand(probs.shape).multinomial(num_samples=1)\n",
    "        \n",
    "        action = action.data\n",
    "        action = action[0]\n",
    "        return action\n",
    "    \n",
    "    def value(self, s):\n",
    "        s = torch.tensor(s)\n",
    "        s = torch.unsqueeze(s, 0)\n",
    "        value, _ = self.forward(s)\n",
    "        value = torch.squeeze(value, 0)\n",
    "        value = value[0]\n",
    "        \n",
    "        return value\n",
    "    \n",
    "def update_weight(optimizer, values, log_probs, rewards, last_value, entropy_term = 0):\n",
    "    # from np.array to torch.tensor\n",
    "    values = torch.stack(values).to(device)\n",
    "    log_probs = torch.stack(log_probs).to(device)\n",
    "    \n",
    "    # compute Q values\n",
    "    Qval = last_value\n",
    "    Qvals = []\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval\n",
    "        Qvals.insert(0, Qval)\n",
    "    \n",
    "    Qvals = torch.stack(Qvals).to(device)\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss = (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd3f1-4262-4944-bc96-ce35e6aa3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 return -543.7500000000059\n",
      "Episode 20 return -535.0000000000069\n",
      "Episode 30 return -550.500000000006\n",
      "Episode 40 return -558.5000000000052\n",
      "Episode 50 return -554.5000000000053\n",
      "Episode 60 return -555.0000000000056\n",
      "Episode 70 return -541.0000000000069\n",
      "Episode 80 return -517.0000000000088\n",
      "Episode 90 return -519.0000000000077\n",
      "Episode 100 return -547.0000000000057\n",
      "Episode 110 return -544.5000000000061\n",
      "Episode 120 return -534.000000000007\n",
      "Episode 130 return -557.5000000000058\n",
      "Episode 140 return -537.5000000000067\n",
      "Episode 150 return -557.0000000000058\n",
      "Episode 160 return -566.5000000000044\n",
      "Episode 170 return -571.0000000000042\n",
      "Episode 180 return -563.0000000000051\n",
      "Episode 190 return -560.0000000000052\n",
      "Episode 200 return -539.0000000000068\n",
      "Episode 210 return -554.0000000000063\n",
      "Episode 220 return -553.0000000000053\n",
      "Episode 230 return -555.5000000000055\n",
      "Episode 240 return -529.0000000000073\n",
      "Episode 250 return -526.0000000000076\n",
      "Episode 260 return -545.5000000000061\n",
      "Episode 270 return -532.0000000000072\n",
      "Episode 280 return -547.500000000006\n",
      "Episode 290 return -546.5000000000057\n",
      "Episode 300 return -531.0000000000076\n",
      "Episode 310 return -531.5000000000073\n",
      "Episode 320 return -548.000000000006\n",
      "Episode 330 return -544.0000000000063\n",
      "Episode 340 return -534.0000000000073\n",
      "Episode 350 return -534.5000000000066\n",
      "Episode 360 return -540.500000000007\n",
      "Episode 370 return -558.000000000005\n",
      "Episode 380 return -532.0000000000067\n",
      "Episode 390 return -543.5000000000064\n",
      "Episode 400 return -561.0000000000051\n",
      "Episode 410 return -530.0000000000075\n",
      "Episode 420 return -538.0000000000067\n",
      "Episode 430 return -548.5000000000058\n",
      "Episode 440 return -549.5000000000056\n",
      "Episode 450 return -529.5000000000073\n",
      "Episode 460 return -533.5000000000073\n",
      "Episode 470 return -540.0000000000072\n",
      "Episode 480 return -546.5000000000058\n",
      "Episode 490 return -527.5000000000076\n",
      "Episode 500 return -510.00000000000904\n",
      "Episode 510 return -541.000000000006\n",
      "Episode 520 return -538.500000000007\n",
      "Episode 530 return -544.5000000000063\n",
      "Episode 540 return -552.0000000000049\n",
      "Episode 550 return -557.5000000000049\n",
      "Episode 560 return -538.500000000007\n",
      "Episode 570 return -511.500000000009\n",
      "Episode 580 return -537.0000000000068\n",
      "Episode 590 return -531.0000000000073\n",
      "Episode 600 return -545.000000000006\n",
      "Episode 610 return -532.500000000007\n",
      "Episode 620 return -533.5000000000065\n",
      "Episode 630 return -528.0000000000075\n",
      "Episode 640 return -532.0000000000072\n",
      "Episode 650 return -530.5000000000069\n",
      "Episode 660 return -548.5000000000055\n",
      "Episode 670 return -531.0000000000074\n",
      "Episode 680 return -546.000000000006\n",
      "Episode 690 return -547.5000000000055\n",
      "Episode 700 return -538.0000000000066\n",
      "Episode 710 return -528.0000000000072\n",
      "Episode 720 return -539.0000000000068\n",
      "Episode 730 return -533.5000000000072\n",
      "Episode 740 return -539.0000000000069\n",
      "Episode 750 return -541.5000000000067\n",
      "Episode 760 return -545.5000000000061\n",
      "Episode 770 return -534.000000000007\n",
      "Episode 780 return -536.5000000000069\n",
      "Episode 790 return -525.0000000000081\n",
      "Episode 800 return -521.5000000000081\n",
      "Episode 810 return -536.5000000000069\n",
      "Episode 820 return -542.5000000000063\n",
      "Episode 830 return -528.0000000000077\n",
      "Episode 840 return -537.0000000000067\n",
      "Episode 850 return -543.0000000000061\n",
      "Episode 860 return -544.0000000000063\n",
      "Episode 870 return -539.5000000000064\n",
      "Episode 880 return -520.5000000000082\n",
      "Episode 890 return -515.0000000000092\n",
      "Episode 900 return -542.0000000000063\n",
      "Episode 910 return -539.5000000000067\n",
      "Episode 920 return -539.0000000000064\n",
      "Episode 930 return -528.0000000000075\n",
      "Episode 940 return -552.5000000000056\n",
      "Episode 950 return -543.5000000000064\n",
      "Episode 960 return -521.5000000000077\n",
      "Episode 970 return -532.0000000000073\n",
      "Episode 980 return -555.0000000000053\n",
      "Episode 990 return -534.500000000007\n",
      "Episode 1000 return -548.0000000000056\n",
      "Episode 1010 return -538.0000000000066\n",
      "Episode 1020 return -551.5000000000059\n",
      "Episode 1030 return -544.0000000000067\n",
      "Episode 1040 return -541.0000000000066\n",
      "Episode 1050 return -547.000000000006\n",
      "Episode 1060 return -532.5000000000068\n",
      "Episode 1070 return -575.0000000000039\n",
      "Episode 1080 return -525.5000000000081\n",
      "Episode 1090 return -539.5000000000063\n",
      "Episode 1100 return -531.000000000007\n",
      "Episode 1110 return -553.0000000000051\n",
      "Episode 1120 return -536.5000000000069\n",
      "Episode 1130 return -532.0000000000074\n",
      "Episode 1140 return -528.0000000000075\n",
      "Episode 1150 return -532.5000000000073\n",
      "Episode 1160 return -561.0000000000052\n",
      "Episode 1170 return -537.0000000000068\n",
      "Episode 1180 return -473.3700000000073\n",
      "Episode 1190 return -525.5000000000077\n",
      "Episode 1200 return -517.0000000000085\n",
      "Episode 1210 return -553.500000000006\n",
      "Episode 1220 return -538.0000000000067\n",
      "Episode 1230 return -544.0000000000063\n",
      "Episode 1240 return -539.5000000000069\n",
      "Episode 1250 return -540.5000000000063\n",
      "Episode 1260 return -545.0000000000066\n",
      "Episode 1270 return -561.5000000000051\n",
      "Episode 1280 return -532.0000000000074\n",
      "Episode 1290 return -550.5000000000056\n",
      "Episode 1300 return -559.0000000000051\n",
      "Episode 1310 return -542.0000000000066\n",
      "Episode 1320 return -541.0000000000064\n",
      "Episode 1330 return -553.0000000000059\n",
      "Episode 1340 return -527.0000000000074\n",
      "Episode 1350 return -545.5000000000059\n",
      "Episode 1360 return -537.0000000000068\n",
      "Episode 1370 return -564.5000000000045\n",
      "Episode 1380 return -539.0000000000068\n",
      "Episode 1390 return -548.0000000000059\n",
      "Episode 1400 return -520.0000000000082\n",
      "Episode 1410 return -542.000000000006\n",
      "Episode 1420 return -522.0000000000078\n",
      "Episode 1430 return -542.0000000000064\n",
      "Episode 1440 return -549.0000000000056\n",
      "Episode 1450 return -536.000000000007\n",
      "Episode 1460 return -542.5000000000061\n",
      "Episode 1470 return -561.0000000000047\n",
      "Episode 1480 return -539.5000000000067\n",
      "Episode 1490 return -547.5000000000057\n",
      "Episode 1500 return -549.000000000006\n",
      "Episode 1510 return -526.5000000000078\n",
      "Episode 1520 return -546.5000000000063\n",
      "Episode 1530 return -542.0000000000063\n",
      "Episode 1540 return -533.0000000000073\n",
      "Episode 1550 return -527.0000000000082\n",
      "Episode 1560 return -551.5000000000056\n",
      "Episode 1570 return -548.0000000000059\n",
      "Episode 1580 return -536.0000000000068\n",
      "Episode 1590 return -542.5000000000066\n",
      "Episode 1600 return -547.5000000000059\n",
      "Episode 1610 return -548.0000000000058\n",
      "Episode 1620 return -533.0000000000075\n",
      "Episode 1630 return -540.0000000000069\n",
      "Episode 1640 return -551.0000000000057\n",
      "Episode 1650 return -539.500000000007\n",
      "Episode 1660 return -552.0000000000055\n",
      "Episode 1670 return -553.0000000000055\n",
      "Episode 1680 return -548.000000000006\n",
      "Episode 1690 return -532.5000000000072\n",
      "Episode 1700 return -540.000000000006\n",
      "Episode 1710 return -539.0000000000069\n",
      "Episode 1720 return -550.0000000000061\n",
      "Episode 1730 return -528.5000000000077\n",
      "Episode 1740 return -555.5000000000051\n",
      "Episode 1750 return -537.5000000000067\n",
      "Episode 1760 return -532.5000000000077\n",
      "Episode 1770 return -532.5000000000074\n",
      "Episode 1780 return -528.5000000000077\n",
      "Episode 1790 return -549.5000000000057\n",
      "Episode 1800 return -532.5000000000074\n",
      "Episode 1810 return -545.0000000000064\n",
      "Episode 1820 return -550.5000000000059\n",
      "Episode 1830 return -537.5000000000072\n",
      "Episode 1840 return -538.0000000000069\n",
      "Episode 1850 return -541.0000000000063\n",
      "Episode 1860 return -541.0000000000065\n",
      "Episode 1870 return -542.0000000000066\n",
      "Episode 1880 return -527.0000000000077\n",
      "Episode 1890 return -545.5000000000065\n",
      "Episode 1900 return -538.5000000000067\n",
      "Episode 1910 return -540.5000000000069\n",
      "Episode 1920 return -535.0000000000069\n",
      "Episode 1930 return -530.5000000000074\n",
      "Episode 1940 return -531.0000000000072\n",
      "Episode 1950 return -542.5000000000058\n",
      "Episode 1960 return -549.5000000000057\n",
      "Episode 1970 return -554.0000000000055\n",
      "Episode 1980 return -545.0000000000063\n",
      "Episode 1990 return -543.0000000000061\n",
      "Episode 2000 return -539.0000000000069\n",
      "Episode 2010 return -524.5000000000082\n",
      "Episode 2020 return -548.0000000000059\n",
      "Episode 2030 return -536.000000000007\n",
      "Episode 2040 return -538.0000000000068\n",
      "Episode 2050 return -523.500000000008\n",
      "Episode 2060 return -539.0000000000067\n",
      "Episode 2070 return -527.5000000000076\n",
      "Episode 2080 return -540.5000000000063\n",
      "Episode 2090 return -546.0000000000056\n",
      "Episode 2100 return -536.500000000007\n",
      "Episode 2110 return -552.5000000000056\n",
      "Episode 2120 return -549.500000000006\n",
      "Episode 2130 return -531.5000000000072\n",
      "Episode 2140 return -539.0000000000067\n",
      "Episode 2150 return -535.5000000000069\n",
      "Episode 2160 return -538.5000000000066\n",
      "Episode 2170 return -553.0000000000056\n",
      "Episode 2180 return -530.5000000000072\n",
      "Episode 2190 return -541.000000000007\n",
      "Episode 2200 return -538.0000000000064\n",
      "Episode 2210 return -547.0000000000059\n",
      "Episode 2220 return -552.5000000000055\n",
      "Episode 2230 return -541.5000000000064\n",
      "Episode 2240 return -540.5000000000065\n",
      "Episode 2250 return -549.000000000006\n",
      "Episode 2260 return -532.000000000007\n",
      "Episode 2270 return -531.500000000007\n",
      "Episode 2280 return -534.5000000000068\n",
      "Episode 2290 return -534.0000000000068\n",
      "Episode 2300 return -552.5000000000053\n",
      "Episode 2310 return -544.0000000000061\n",
      "Episode 2320 return -542.5000000000064\n",
      "Episode 2330 return -523.5000000000076\n",
      "Episode 2340 return -541.0000000000068\n",
      "Episode 2350 return -553.0000000000061\n",
      "Episode 2360 return -535.0000000000069\n",
      "Episode 2370 return -534.5000000000069\n",
      "Episode 2380 return -523.0000000000082\n",
      "Episode 2390 return -549.0000000000059\n",
      "Episode 2400 return -539.0000000000066\n",
      "Episode 2410 return -533.0000000000069\n",
      "Episode 2420 return -541.5000000000067\n",
      "Episode 2430 return -534.0000000000073\n",
      "Episode 2440 return -540.5000000000063\n",
      "Episode 2450 return -544.5000000000059\n",
      "Episode 2460 return -592.5000000000006\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "ALPHA = 3e-4 # learning rate\n",
    "GAMMA = 0.99 # step-size\n",
    "\n",
    "env = MazeSolverEnv()\n",
    "\n",
    "num_actions = env.num_action\n",
    "num_states = env.num_obs\n",
    "\n",
    "actor_critic = ActorCritic(num_states[0], num_states[1], num_actions).to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=ALPHA)\n",
    "\n",
    "try:\n",
    "    returns = []\n",
    "    \n",
    "    for i_episode in range(MAX_EPISODES):\n",
    "\n",
    "        state = env.init_obs\n",
    "        init_state = state\n",
    "        \n",
    "        #state = env.init_obs\n",
    "        done = False\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []   # no reward at t = 0\n",
    "\n",
    "        #while not done:\n",
    "        for timesteps in range(MAX_TIMESTEPS):\n",
    "            value = actor_critic.value(state)\n",
    "            values.append(value)\n",
    "            \n",
    "            action = actor_critic.get_action(state)\n",
    "            log_prob = torch.log(actor_critic.pi(state, action))\n",
    "            log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action.tolist())\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done or timesteps == MAX_TIMESTEPS-1:\n",
    "                last_Qval = actor_critic.value(state)\n",
    "                break\n",
    "        \n",
    "        update_weight(optimizer, values, log_probs, rewards, last_Qval)\n",
    "        \n",
    "        '''\n",
    "        #====================================================================================================\n",
    "        #to see the change of the weights====================================================================\n",
    "        #====================================================================================================\n",
    "        print(\"=========================================================================================\")\n",
    "        print(\"actor_critic.actor_fc1.weight : {}\".format(actor_critic.actor_conv1.weight))\n",
    "        print(\"actor_critic.critic_fc2.weight : {}\".format(actor_critic.critic_conv1.weight))\n",
    "        #====================================================================================================\n",
    "        '''\n",
    "\n",
    "        if (i_episode + 1) % 500 == 0:\n",
    "            print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "            returns.append(sum(rewards))\n",
    "            \n",
    "            torch.save(actor_critic, './saved_models/actor_critic' + str(i_episode + 1) + '.pt')\n",
    "            \n",
    "        elif (i_episode + 1) % 10 == 0:\n",
    "            print(\"Episode {} return {}\".format(i_episode + 1, sum(rewards)))\n",
    "            returns.append(sum(rewards))\n",
    "            \n",
    "        # every 10th episodes, turn off the exploring starts\n",
    "        if (i_episode + 2) % 10 == 0:\n",
    "            env.reset_player(exploring_starts = False)\n",
    "        else:\n",
    "            env.reset_player(exploring_starts = True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "finally:\n",
    "    plt.plot([i for i in range(10, len(returns)+1, 10)], returns)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
