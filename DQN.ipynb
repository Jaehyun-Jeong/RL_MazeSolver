{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import Maze_Solver as maze_solver\n",
    "from Maze_Solver import MazeSolver, MazeSolverEnv\n",
    "import Maze_Generator as maze_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeSolverEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from typing import Dict, NamedTuple, List\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cuda가 설치되어 있다면 cuda를 사용하고 아니라면 cpu만을 사용한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "    obs_pred: torch.tensor\n",
    "    next_obs_pred: torch.tensor\n",
    "\n",
    "# 하나의 에피소드에서 모든 Experience를 저장한다.\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# Trajectory에서 일정한 크기의 Experience를 임의로 뽑아낸다.\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding_mode='zeros')\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding_mode='zeros')\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding_mode='zeros')\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.fc = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Softmax(dim=1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    # generate_trajectory는 학습 전에 쓰이는 데이터를 수집한다.\n",
    "    @staticmethod\n",
    "    def generate_trajectory(\n",
    "        q_net: model, buffer_size: int, step_size: int, epsilon: float\n",
    "    ):\n",
    "        # 이는 Replay Memory를 사용하기 위한 버퍼이다.\n",
    "        buffer: Buffer = []\n",
    "\n",
    "        # 환경으로부터 초기 상태 값을 가져온다.\n",
    "        observation = env.init_obs\n",
    "        # initial state prediction\n",
    "        obs_prediction = q_net(torch.from_numpy(np.copy(observation)).to(device))\n",
    "\n",
    "        # 에이전트의 Transition을 저장한다.\n",
    "        trajectory_from_agent: [Trajectory] = []\n",
    "\n",
    "        # 환경의 전 상태값을 저장한다.\n",
    "        last_obs_from_agent = np.ndarray\n",
    "\n",
    "        # Return을 계산하기 위해 축적된 보상을 저장한다.\n",
    "        cumulative_reward_from_agent = 0\n",
    "\n",
    "        # Return들을 저장한다.\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        # 설정의 버퍼의 크기 만큼만 버퍼에 Transition을 저장한다.\n",
    "        while len(buffer) < buffer_size:\n",
    "            #=============================================\n",
    "            # 행동을 선택한다.\n",
    "            #=============================================\n",
    "            action_values = (\n",
    "                obs_prediction.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = np.argmax(action_values)\n",
    "            else:\n",
    "                final_action_idx = np.size(action_values) - 1\n",
    "                action_idx = random.randint(0, final_action_idx)\n",
    "\n",
    "            action = [action_idx]\n",
    "            #=============================================\n",
    "\n",
    "            # 에이전트가 한번의 step을 진행한다.\n",
    "            next_observation, reward, done, _ = env.step(action_idx)\n",
    "            \n",
    "            next_obs_prediction = q_net(torch.from_numpy(next_observation).to(device))\n",
    "\n",
    "            # 만약 Terminal 상태에 도달하지 않았다면 다음 코드를 실행한다.\n",
    "            if not done:\n",
    "                exp = Experience(\n",
    "                    obs = np.copy(observation),\n",
    "                    reward = reward,\n",
    "                    done = False,\n",
    "                    action = np.copy(action),\n",
    "                    next_obs = np.copy(next_observation),\n",
    "                    obs_pred = torch.clone(obs_prediction),\n",
    "                    next_obs_pred = torch.clone(next_obs_prediction)\n",
    "                )\n",
    "\n",
    "                cumulative_reward_from_agent += reward\n",
    "\n",
    "                trajectory_from_agent.append(exp)\n",
    "                observation = np.copy(next_observation)\n",
    "                obs_prediction = torch.clone(next_obs_prediction)\n",
    "\n",
    "            # 만약 Terminal 상태에 도달했다면 다음 코드를 실행한다.\n",
    "            else:\n",
    "                last_experience = Experience(\n",
    "                    obs = np.copy(observation),\n",
    "                    reward = reward,\n",
    "                    done = done,\n",
    "                    action = np.copy(action),\n",
    "                    next_obs = np.copy(next_observation),\n",
    "                    obs_pred = torch.clone(obs_prediction),\n",
    "                    next_obs_pred = torch.clone(next_obs_prediction)\n",
    "                )\n",
    "\n",
    "                cumulative_reward = cumulative_reward_from_agent + reward\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                cumulative_reward_from_agent = 0\n",
    "\n",
    "                buffer.extend(trajectory_from_agent)\n",
    "                buffer.append(last_experience)\n",
    "\n",
    "                # 하나의 Trajectory가 수집되었으므로 환경을 초기화하고 다시 반복한다.\n",
    "                env.reset()\n",
    "\n",
    "\n",
    "        # 얻어진 데이터와 보상 축적값의 평균값을 리턴한다.\n",
    "        return buffer.copy(), np.mean(cumulative_rewards)\n",
    "    \n",
    "    # update_q_net은 뉴럴 네트워크에 대하여 Gradient Descent를 진행한다.\n",
    "    @staticmethod\n",
    "    def update_q_net(\n",
    "        q_net: model,\n",
    "        optimizer: torch.optim,\n",
    "        buffer: Buffer,\n",
    "        action_size = int\n",
    "    ):\n",
    "        \n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "        \n",
    "            BATCH_SIZE = 1000\n",
    "            NUM_EPOCH = 3\n",
    "            GAMMA = 0.9\n",
    "            batch_size = min(len(buffer), BATCH_SIZE)\n",
    "            random.shuffle(buffer)\n",
    "\n",
    "            # 버퍼에서 배치의 크기만큼 데이터를 추출한다.\n",
    "            batches = [\n",
    "                buffer[batch_size * start : batch_size * (start + 1)]\n",
    "                for start in range(int(len(buffer) / batch_size))\n",
    "            ]\n",
    "\n",
    "            for _ in range(NUM_EPOCH):\n",
    "                for batch in batches:\n",
    "                    #obs = torch.from_numpy(np.stack([ex.obs for ex in batch])).to(device)\n",
    "                    obs_pred = torch.stack([ex.obs_pred[0] for ex in batch]).to(device)\n",
    "                    reward = torch.from_numpy(\n",
    "                        np.array([ex.reward for ex in batch], dtype = np.float32).reshape(-1, 1)\n",
    "                    ).to(device)\n",
    "                    done = torch.from_numpy(\n",
    "                        np.array([ex.done for ex in batch], dtype = np.float32).reshape(-1, 1)\n",
    "                    ).to(device)\n",
    "                    action = torch.from_numpy(np.stack([ex.action for ex in batch])).to(device)\n",
    "                    #next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "                    next_obs_pred = torch.stack([ex.next_obs_pred[0] for ex in batch]).to(device)\n",
    "                    \n",
    "                    obs_pred, reward, done, action, next_obs_pred = torch.clone(obs_pred), torch.clone(reward), torch.clone(done), torch.clone(action), torch.clone(next_obs_pred)\n",
    "\n",
    "                    # 전 챕터에서 설명한 Deep Q-learning의 타겟이다.\n",
    "                    target = (\n",
    "                        reward\n",
    "                        + (1.0 - done)\n",
    "                        * GAMMA\n",
    "                        * torch.max(next_obs_pred, dim=1, keepdim=True).values\n",
    "                    )\n",
    "\n",
    "                    #=============================================\n",
    "                    # 뉴럴 네트워크의 에러를 구한다.\n",
    "                    #=============================================\n",
    "                    mask = torch.zeros((len(batch), action_size)).to(device)\n",
    "                    mask = mask.scatter(1, action, 1)\n",
    "                    \n",
    "                    mask = torch.clone(mask)\n",
    "\n",
    "                    prediction = torch.sum(obs_pred * mask, dim = 1, keepdim = True)\n",
    "                    \n",
    "                    prediction = torch.clone(prediction)\n",
    "                    target = torch.clone(target)\n",
    "                    \n",
    "                    criterion = nn.SmoothL1Loss()\n",
    "                    #criterion = nn.MSELoss()\n",
    "                    loss = criterion(prediction, target)\n",
    "                    #=============================================\n",
    "\n",
    "                    #=============================================\n",
    "                    # Gradient Descent\n",
    "                    #=============================================\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    #============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer.update_q_net(qnet, optim, new_exp, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted\n",
      "\n",
      "Plot failed on interrupted training.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=============================================\n",
    "# 학습에 필요한 값들을 초기화한다.\n",
    "#=============================================\n",
    "env.reset()\n",
    "\n",
    "num_actions = env.num_action\n",
    "num_obs = env.num_obs\n",
    "\n",
    "# 아래 3개의 변수는 엡실론 스케쥴링을 위해 필요하다.\n",
    "EPS_START = 0.01\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "#=============================================\n",
    "\n",
    "try:\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        qnet = model(num_obs[0], num_obs[1], num_actions)\n",
    "        qnet.to(device)\n",
    "\n",
    "        experiences: Buffer = []\n",
    "        optim = torch.optim.Adam(qnet.parameters(), lr = 0.001)\n",
    "\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 5000))\n",
    "        NUM_NEW_EXP = int(os.getenv(\"QLEARNING_NUM_NEW_EXP\", 100))\n",
    "        NUM_NEW_STEP = int(os.getenv(\"QLEARNING_NUM_NEW_EXP\", 100))\n",
    "        BUFFER_SIZE = int(os.getenv(\"QLEARNING_BUFFER_SIZE\", 500))\n",
    "\n",
    "        for n in range(NUM_TRAINING_STEPS):\n",
    "            # 엡실론을 설정한다.\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * n /EPS_DECAY)\n",
    "\n",
    "            # 학습에 사용될 데이터를 가져온다.\n",
    "            new_exp, _ = Trainer.generate_trajectory(qnet, NUM_NEW_EXP, NUM_NEW_STEP, epsilon = eps_threshold)\n",
    "\n",
    "            # 전 챕터에서 언급한 데이터의 독립성을 보장하기 위해 필요하다.\n",
    "            random.shuffle(experiences)\n",
    "\n",
    "            # 버퍼 사이즈 만큼만 데이터를 수집한다.\n",
    "            if len(experiences) > BUFFER_SIZE:\n",
    "                experiences = experiences[:BUFFER_SIZE]\n",
    "            experiences.extend(new_exp)\n",
    "\n",
    "            # 학습을 진행하고 보상값을 저장 및 출력한다.\n",
    "            Trainer.update_q_net(qnet, optim, new_exp, num_actions)\n",
    "            _, rewards = Trainer.generate_trajectory(qnet, 100, NUM_NEW_STEP, epsilon = 0)\n",
    "            cumulative_rewards.append(rewards)\n",
    "            print(f\"Training step: {n+1}\\treward: {rewards}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted\")\n",
    "finally:\n",
    "    env.close()\n",
    "\n",
    "# 학습이 끝나면 보상의 변화를 그래프로 표현하고 모델을 저장한다.\n",
    "try:\n",
    "    plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n",
    "    \n",
    "    # SAVE MODEL\n",
    "    torch.save(qnet, './saved_models/model.pt')\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"\\nPlot failed on interrupted training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
